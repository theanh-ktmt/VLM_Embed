python train_distillation.py --model_name llava-hf/llava-onevision-qwen2-0.5b-ov-hf --student_model_path llava-hf/llava-onevision-qwen2-0.5b-ov-hf --train_student_lora True --student_lora_r 4 --student_backbone llava_onevision --student_pooling eos --teacher_model_path raghavlite/B3_Qwen2_2B --teacher_lora True --teacher_lora_r 8 --teacher_backbone qwen2_vl --teacher_pooling eos --dataset_name TIGER-Lab/MMEB-train --subset_name HatefulMemes VOC2007 --dataset_split original --image_dir vlm2vec_train/MMEB-train --output_dir training/distill_B2_Qwen2_2B_v2 --per_device_train_batch_size 2 --gradient_accumulation_steps 2 --learning_rate 1e-4 --num_train_epochs 1 --bf16 --save_total_limit 2 --logging_steps 1 --save_strategy epoch --seed 42 --weight_decay 0.01 --remove_unused_columns True --normalize True