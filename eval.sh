python  eval_mmeb.py  --model_name /workspace/ComfyUI/models/gligen/VLM_Embed/training/no_deepspeed_propose_kd_weight/checkpoint-final --encode_output_path  ./MMEB-evaloutputs/propose_2/  --pooling  eos  --normalize  True  --lora --lora_r 32 --bf16  --dataset_name  TIGER-Lab/MMEB-eval  --subset_name  HatefulMemes --dataset_split  test  --per_device_eval_batch_size  8  --image_dir  eval_images/ --image_resolution low --tgt_prefix_mod
# python  eval_mmeb.py  --model_name raghavlite/B3_Qwen2_2B --encode_output_path  ./encoded_data/B2_Qwen2_2B_0/  --pooling  eos  --normalize  True  --lora  --lora_r  8  --bf16  --dataset_name  TIGER-Lab/MMEB-eval  --subset_name  HatefulMemes --dataset_split  test  --per_device_eval_batch_size  4  --image_dir  ./eval_images --tgt_prefix_mod
# rm -rf ./MMEB-evaloutputs/B2_Qwen2_2B_v0/
# python  eval_mmeb.py  --model_name llava-hf/llava-onevision-qwen2-0.5b-ov-hf --encode_output_path  ./MMEB-evaloutputs/llava-0.5B_raw/  --pooling  eos  --normalize  True  --bf16  --dataset_name  TIGER-Lab/MMEB-eval  --subset_name  HatefulMemes --dataset_split  test  --per_device_eval_batch_size  4  --image_dir  eval_images/ --image_resolution low