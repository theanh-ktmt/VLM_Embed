python  eval_mmeb.py  --model_name apple/FastVLM-0.5B --encode_output_path  ./MMEB-evaloutputs/rkd/  --pooling  eos  --normalize  True  --bf16  --dataset_name  TIGER-Lab/MMEB-eval  --subset_name HatefulMemes ImageNet-1K --dataset_split  test  --per_device_eval_batch_size  64  --image_dir  eval_images/ 
# python  eval_mmeb.py  --model_name raghavlite/B3_Qwen2_2B --encode_output_path  ./encoded_data/B2_Qwen2_2B_0/  --pooling  eos  --normalize  True  --lora  --lora_r  8  --bf16  --dataset_name  TIGER-Lab/MMEB-eval  --subset_name  HatefulMemes --dataset_split  test  --per_device_eval_batch_size  4  --image_dir  ./eval_images --tgt_prefix_mod
# rm -rf ./MMEB-evaloutputs/B2_Qwen2_2B_v0/
# python  eval_mmeb.py  --model_name llava-hf/llava-onevision-qwen2-0.5b-ov-hf --encode_output_path  ./MMEB-evaloutputs/llava-0.5B_raw/  --pooling  eos  --normalize  True  --bf16  --dataset_name  TIGER-Lab/MMEB-eval  --subset_name  HatefulMemes --dataset_split  test  --per_device_eval_batch_size  4  --image_dir  eval_images/ --image_resolution low